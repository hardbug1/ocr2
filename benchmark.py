#!/usr/bin/env python3
"""
Performance Benchmark Tool for Korean OCR
PRD 4.2 ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ê²€ì¦
"""

import time
import psutil
import os
import json
from typing import Dict, List, Tuple
from pathlib import Path
import matplotlib.pyplot as plt
import pandas as pd

from main import KoreanOCRPipeline
from advanced_postprocessor import KoreanPostProcessor

class OCRBenchmark:
    """OCR ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬"""
    
    def __init__(self):
        self.results = []
        self.system_info = self._get_system_info()
        
    def _get_system_info(self) -> Dict:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        
        import platform
        import torch
        
        info = {
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'cpu_count': psutil.cpu_count(),
            'memory_gb': round(psutil.virtual_memory().total / (1024**3), 2),
            'gpu_available': torch.cuda.is_available(),
            'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
        }
        
        return info
    
    def run_speed_benchmark(self, image_paths: List[str], use_yolo: bool = False) -> Dict:
        """ì²˜ë¦¬ ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        
        print(f"ğŸš€ ì†ë„ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (YOLO: {use_yolo})")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìˆ˜: {len(image_paths)}")
        
        # OCR íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = KoreanOCRPipeline(use_yolo=use_yolo)
        
        speeds = []
        memory_usage = []
        
        for i, image_path in enumerate(image_paths):
            print(f"ì²˜ë¦¬ ì¤‘... {i+1}/{len(image_paths)}: {Path(image_path).name}")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            
            try:
                result = pipeline.process_image(image_path)
                end_time = time.time()
                
                processing_time = end_time - start_time
                speeds.append(processing_time)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
                memory_after = process.memory_info().rss / 1024 / 1024  # MB
                memory_used = memory_after - memory_before
                memory_usage.append(memory_used)
                
                print(f"  âœ… ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ, ë©”ëª¨ë¦¬: {memory_used:.1f}MB")
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}")
                speeds.append(None)
                memory_usage.append(None)
        
        # í†µê³„ ê³„ì‚°
        valid_speeds = [s for s in speeds if s is not None]
        valid_memory = [m for m in memory_usage if m is not None]
        
        benchmark_result = {
            'device': 'YOLO' if use_yolo else 'Ensemble',
            'total_images': len(image_paths),
            'successful_images': len(valid_speeds),
            'failed_images': len(image_paths) - len(valid_speeds),
            'avg_speed': sum(valid_speeds) / len(valid_speeds) if valid_speeds else 0,
            'min_speed': min(valid_speeds) if valid_speeds else 0,
            'max_speed': max(valid_speeds) if valid_speeds else 0,
            'avg_memory_mb': sum(valid_memory) / len(valid_memory) if valid_memory else 0,
            'max_memory_mb': max(valid_memory) if valid_memory else 0,
            'speeds': speeds,
            'memory_usage': memory_usage,
        }
        
        return benchmark_result
    
    def run_accuracy_benchmark(self, test_cases: List[Dict]) -> Dict:
        """ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬"""
        
        print("ğŸ¯ ì •í™•ë„ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        pipeline = KoreanOCRPipeline()
        postprocessor = KoreanPostProcessor()
        
        accuracy_results = []
        
        for i, test_case in enumerate(test_cases):
            image_path = test_case['image']
            expected_text = test_case['expected']
            
            print(f"í…ŒìŠ¤íŠ¸ {i+1}/{len(test_cases)}: {Path(image_path).name}")
            
            try:
                # OCR ì‹¤í–‰
                result = pipeline.process_image(image_path)
                ocr_text = result['text']
                
                # í›„ì²˜ë¦¬ ì ìš©
                processed_text = postprocessor.process(ocr_text)
                
                # ì •í™•ë„ ê³„ì‚°
                accuracy_raw = self._calculate_accuracy(expected_text, ocr_text)
                accuracy_processed = self._calculate_accuracy(expected_text, processed_text)
                
                accuracy_results.append({
                    'image': image_path,
                    'expected': expected_text,
                    'ocr_raw': ocr_text,
                    'ocr_processed': processed_text,
                    'accuracy_raw': accuracy_raw,
                    'accuracy_processed': accuracy_processed,
                })
                
                print(f"  ğŸ“Š ì›ë³¸ ì •í™•ë„: {accuracy_raw:.1f}%")
                print(f"  ğŸ“Š í›„ì²˜ë¦¬ ì •í™•ë„: {accuracy_processed:.1f}%")
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}")
                accuracy_results.append({
                    'image': image_path,
                    'expected': expected_text,
                    'error': str(e),
                })
        
        # ì „ì²´ ì •í™•ë„ ê³„ì‚°
        valid_results = [r for r in accuracy_results if 'accuracy_raw' in r]
        
        benchmark_result = {
            'total_cases': len(test_cases),
            'successful_cases': len(valid_results),
            'avg_accuracy_raw': sum(r['accuracy_raw'] for r in valid_results) / len(valid_results) if valid_results else 0,
            'avg_accuracy_processed': sum(r['accuracy_processed'] for r in valid_results) / len(valid_results) if valid_results else 0,
            'detailed_results': accuracy_results,
        }
        
        return benchmark_result
    
    def _calculate_accuracy(self, expected: str, actual: str) -> float:
        """ë¬¸ìì—´ ì •í™•ë„ ê³„ì‚° (Levenshtein distance ê¸°ë°˜)"""
        
        from difflib import SequenceMatcher
        
        # ê³µë°± ì •ê·œí™”
        expected = ' '.join(expected.split())
        actual = ' '.join(actual.split())
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity = SequenceMatcher(None, expected, actual).ratio()
        
        return similarity * 100
    
    def run_prd_compliance_test(self, test_images: List[str]) -> Dict:
        """PRD ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸"""
        
        print("ğŸ“‹ PRD ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸")
        
        # ì†ë„ í…ŒìŠ¤íŠ¸
        speed_ensemble = self.run_speed_benchmark(test_images, use_yolo=False)
        speed_yolo = self.run_speed_benchmark(test_images, use_yolo=True)
        
        # PRD ìš”êµ¬ì‚¬í•­ ì²´í¬
        prd_requirements = {
            'processing_speed_ensemble': {
                'requirement': 5.0,  # 5ì´ˆ/í˜ì´ì§€
                'actual': speed_ensemble['avg_speed'],
                'passed': speed_ensemble['avg_speed'] <= 5.0,
                'unit': 'ì´ˆ/í˜ì´ì§€'
            },
            'processing_speed_yolo': {
                'requirement': 3.0,  # 3ì´ˆ/í˜ì´ì§€ (YOLO ëª©í‘œ)
                'actual': speed_yolo['avg_speed'],
                'passed': speed_yolo['avg_speed'] <= 3.0,
                'unit': 'ì´ˆ/í˜ì´ì§€'
            },
            'memory_usage': {
                'requirement': 2048,  # 2GB
                'actual': max(speed_ensemble['max_memory_mb'], speed_yolo['max_memory_mb']),
                'passed': max(speed_ensemble['max_memory_mb'], speed_yolo['max_memory_mb']) <= 2048,
                'unit': 'MB'
            },
            'success_rate': {
                'requirement': 95.0,  # 95% ì„±ê³µë¥ 
                'actual': (speed_ensemble['successful_images'] / speed_ensemble['total_images']) * 100,
                'passed': (speed_ensemble['successful_images'] / speed_ensemble['total_images']) * 100 >= 95.0,
                'unit': '%'
            }
        }
        
        return {
            'system_info': self.system_info,
            'speed_ensemble': speed_ensemble,
            'speed_yolo': speed_yolo,
            'prd_compliance': prd_requirements,
            'overall_passed': all(req['passed'] for req in prd_requirements.values())
        }
    
    def generate_report(self, results: Dict, output_path: str = "benchmark_report.json"):
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ì½˜ì†” ë¦¬í¬íŠ¸ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸")
        print("="*60)
        
        print(f"ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   - í”Œë«í¼: {self.system_info['platform']}")
        print(f"   - CPU: {self.system_info['cpu_count']}ì½”ì–´")
        print(f"   - ë©”ëª¨ë¦¬: {self.system_info['memory_gb']}GB")
        print(f"   - GPU: {self.system_info['gpu_name'] or 'N/A'}")
        
        if 'prd_compliance' in results:
            print(f"\nğŸ“‹ PRD ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜:")
            for name, req in results['prd_compliance'].items():
                status = "âœ… PASS" if req['passed'] else "âŒ FAIL"
                print(f"   - {name}: {req['actual']:.2f}{req['unit']} (ìš”êµ¬: {req['requirement']}{req['unit']}) {status}")
        
        print(f"\nğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸: {output_path}")
        print("="*60)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    benchmark = OCRBenchmark()
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ
    test_images = [
        "2.jpg",
        "test_korean.jpg",
        # ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤...
    ]
    
    # ì¡´ì¬í•˜ëŠ” ì´ë¯¸ì§€ë§Œ í•„í„°ë§
    existing_images = [img for img in test_images if os.path.exists(img)]
    
    if existing_images:
        print(f"ğŸ§ª {len(existing_images)}ê°œ ì´ë¯¸ì§€ë¡œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
        
        # PRD ì¤€ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = benchmark.run_prd_compliance_test(existing_images)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        benchmark.generate_report(results)
        
    else:
        print("âŒ í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ì´ë¯¸ì§€ ì¤‘ í•˜ë‚˜ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”:")
        for img in test_images:
            print(f"  - {img}") 