#!/usr/bin/env python3
"""
GPU í™˜ê²½ ê°ì§€ ë° ìë™ ì„¤ì • ìœ í‹¸ë¦¬í‹°
Mac Apple Silicon (MPS) ë° NVIDIA CUDA ì§€ì›
"""

import torch
import platform
import subprocess
import sys

class GPUConfig:
    """GPU í™˜ê²½ ê°ì§€ ë° ì„¤ì • ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.gpu_info = self._detect_gpu_environment()
    
    def _detect_gpu_environment(self):
        """ëª¨ë“  GPU í™˜ê²½ ê°ì§€ (CUDA, MPS, CPU)"""
        
        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        system_info = {
            'platform': platform.system(),
            'machine': platform.machine(),
            'python_version': sys.version
        }
        
        # CUDA ì§€ì› í™•ì¸
        cuda_available = torch.cuda.is_available()
        cuda_device_count = torch.cuda.device_count() if cuda_available else 0
        cuda_device_name = torch.cuda.get_device_name(0) if cuda_available else None
        
        # MPS (Apple Silicon) ì§€ì› í™•ì¸
        mps_available = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False
        mps_built = torch.backends.mps.is_built() if hasattr(torch.backends, 'mps') else False
        
        # Apple Silicon ê°ì§€
        is_apple_silicon = (
            system_info['platform'] == 'Darwin' and 
            system_info['machine'] in ['arm64', 'aarch64']
        )
        
        return {
            'system_info': system_info,
            'cuda': {
                'available': cuda_available,
                'device_count': cuda_device_count,
                'device_name': cuda_device_name
            },
            'mps': {
                'available': mps_available,
                'built': mps_built,
                'is_apple_silicon': is_apple_silicon
            }
        }
    
    def is_available(self):
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return self.gpu_info['cuda']['available'] or self.gpu_info['mps']['available']
    
    def get_gpu_type(self):
        """GPU íƒ€ì… ë°˜í™˜"""
        if self.gpu_info['cuda']['available']:
            return 'cuda'
        elif self.gpu_info['mps']['available']:
            return 'mps'
        else:
            return 'cpu'
    
    def get_device(self):
        """PyTorch ë””ë°”ì´ìŠ¤ ê°ì²´ ë°˜í™˜"""
        gpu_type = self.get_gpu_type()
        
        if gpu_type == 'cuda':
            return torch.device('cuda')
        elif gpu_type == 'mps':
            return torch.device('mps')
        else:
            return torch.device('cpu')
    
    def get_optimal_settings(self):
        """ìµœì  OCR ì„¤ì • ë°˜í™˜"""
        
        gpu_type = self.get_gpu_type()
        
        settings = {
            'use_gpu': self.is_available(),
            'gpu_type': gpu_type,
            'torch_device': self.get_device(),
            'easyocr_gpu': self.is_available(),
            'paddleocr_use_gpu': self.gpu_info['cuda']['available'],  # PaddleOCRëŠ” CUDAë§Œ ì§€ì›
            'batch_size': self._get_optimal_batch_size(),
            'max_workers': self._get_optimal_workers()
        }
        
        return settings
    
    def _get_optimal_batch_size(self):
        """GPU íƒ€ì…ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸°"""
        gpu_type = self.get_gpu_type()
        
        if gpu_type == 'cuda':
            return 8  # NVIDIA GPUëŠ” ë†’ì€ ë°°ì¹˜ í¬ê¸° ì§€ì›
        elif gpu_type == 'mps':
            return 4  # Apple Siliconì€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ í¬ê¸°
        else:
            return 2  # CPUëŠ” ë‚®ì€ ë°°ì¹˜ í¬ê¸°
    
    def _get_optimal_workers(self):
        """GPU íƒ€ì…ì— ë”°ë¥¸ ìµœì  ì›Œì»¤ ìˆ˜"""
        gpu_type = self.get_gpu_type()
        
        if gpu_type in ['cuda', 'mps']:
            return 4  # GPU ì‚¬ìš© ì‹œ ë³‘ë ¬ ì²˜ë¦¬
        else:
            return 2  # CPU ì‚¬ìš© ì‹œ ì œí•œì  ë³‘ë ¬ ì²˜ë¦¬
    
    def print_gpu_info(self):
        """GPU ì •ë³´ ì¶œë ¥"""
        print("ğŸ”§ GPU í™˜ê²½ ì •ë³´:")
        print(f"   ì‹œìŠ¤í…œ: {self.gpu_info['system_info']['platform']} ({self.gpu_info['system_info']['machine']})")
        
        # CUDA ì •ë³´
        if self.gpu_info['cuda']['available']:
            print(f"   âœ… CUDA: ì‚¬ìš© ê°€ëŠ¥")
            print(f"      ë””ë°”ì´ìŠ¤: {self.gpu_info['cuda']['device_name']}")
            print(f"      ë””ë°”ì´ìŠ¤ ìˆ˜: {self.gpu_info['cuda']['device_count']}")
        else:
            print(f"   âŒ CUDA: ì‚¬ìš© ë¶ˆê°€")
        
        # MPS ì •ë³´
        if self.gpu_info['mps']['is_apple_silicon']:
            if self.gpu_info['mps']['available']:
                print(f"   âœ… MPS (Apple Silicon): ì‚¬ìš© ê°€ëŠ¥")
            else:
                print(f"   âš ï¸  MPS (Apple Silicon): ê°ì§€ë˜ì—ˆìœ¼ë‚˜ ì‚¬ìš© ë¶ˆê°€")
                if not self.gpu_info['mps']['built']:
                    print(f"      PyTorch MPS ì§€ì›ì´ ë¹Œë“œë˜ì§€ ì•ŠìŒ")
        
        # ì„ íƒëœ ë””ë°”ì´ìŠ¤
        selected_device = self.get_gpu_type()
        print(f"   ğŸ¯ ì„ íƒëœ ë””ë°”ì´ìŠ¤: {selected_device.upper()}")
        
        # ìµœì  ì„¤ì •
        settings = self.get_optimal_settings()
        print(f"   ğŸ“Š ìµœì  ì„¤ì •:")
        print(f"      ë°°ì¹˜ í¬ê¸°: {settings['batch_size']}")
        print(f"      ì›Œì»¤ ìˆ˜: {settings['max_workers']}")
        print(f"      EasyOCR GPU: {settings['easyocr_gpu']}")
        print(f"      PaddleOCR GPU: {settings['paddleocr_use_gpu']}")

def detect_gpu_availability():
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    
    gpu_config = GPUConfig()
    gpu_info = gpu_config.gpu_info
    
    return {
        'gpu_available': gpu_config.is_available(),
        'device_count': gpu_info['cuda']['device_count'] if gpu_info['cuda']['available'] else 0,
        'device_name': gpu_info['cuda']['device_name'] if gpu_info['cuda']['available'] else 'MPS' if gpu_info['mps']['available'] else None,
        'gpu_type': gpu_config.get_gpu_type()
    }

def get_optimal_settings():
    """ìµœì  ì„¤ì • ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    
    gpu_config = GPUConfig()
    settings = gpu_config.get_optimal_settings()
    
    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ í˜•ì‹ ë³€í™˜
    legacy_settings = {
        'use_gpu': settings['use_gpu'],
        'easyocr_gpu': settings['easyocr_gpu'],
        'torch_device': str(settings['torch_device'])
    }
    
    gpu_config.print_gpu_info()
    
    return legacy_settings

# ë©”ì¸ ì‹¤í–‰ ì‹œ GPU ì •ë³´ ì¶œë ¥
if __name__ == "__main__":
    gpu_config = GPUConfig()
    gpu_config.print_gpu_info()
    
    print("\nğŸ§ª GPU í…ŒìŠ¤íŠ¸:")
    device = gpu_config.get_device()
    print(f"   PyTorch ë””ë°”ì´ìŠ¤: {device}")
    
    # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸
    try:
        test_tensor = torch.randn(10, 10).to(device)
        result = torch.matmul(test_tensor, test_tensor)
        print(f"   âœ… í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result.shape}")
    except Exception as e:
        print(f"   âŒ í…ì„œ ì—°ì‚° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    print("\nâš™ï¸  ê¶Œì¥ OCR ì„¤ì •:")
    settings = gpu_config.get_optimal_settings()
    for key, value in settings.items():
        print(f"   {key}: {value}") 